# Estagio_CBL_Ci√™nciaDeDados_HDL

üéØ Prot√≥tipo de pipeline de dados (ETL/ELT) desenvolvido com Python, Pandas, SQL e GIT, seguindo a metodologia Challenge-Based Learning (CBL) para est√°gio em Ci√™ncia de Dados(HDL).

# üöÄ Fase 1: Engajamento (Big Idea & Essential Question)

Definir o escopo do desafio de aprendizado, alinhando-o com os objetivos da vaga.

Este projeto foi desenhado para o processo seletivo de Est√°gio em Ci√™ncia de Dados na HDL, focando na constru√ß√£o de um sistema de informa√ß√£o robusto.

## Ideia Central (Big Idea):

O Valor dos Dados e a L√≥gica de Sistema: Como os Dados Brutos s√£o Transformados em Insights Acion√°veis e Conhecimento Estrat√©gico para a Empresa.

Analogia HDL: Assim como uma Hardware Description Language (HDL) define a arquitetura l√≥gica e o comportamento eficiente de um chip de computador, o pipeline de dados em Python deve definir a arquitetura l√≥gica para transformar dados brutos em intelig√™ncia, garantindo efici√™ncia e precis√£o em cada etapa do fluxo.

## Pergunta Essencial (Essential Question):

Como posso desenvolver um fluxo de trabalho de ponta a ponta em Python, utilizando pr√°ticas de engenharia de software (Git), para transformar dados heterog√™neos em um dataset limpo e pronto para an√°lise, gerando um insight estrat√©gico que promova uma melhoria processual na √°rea de atua√ß√£o?

## O Desafio (Challenge):

Construir um mini-pipeline de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) para processar dados de vendas (simulados: CSV, JSON, consulta SQL) e gerar um painel de indicadores (dashboard) simples em Python, que revele a principal 'dor' ou oportunidade de otimiza√ß√£o de processo na opera√ß√£o de uma empresa hipot√©tica (ex: log√≠stica, marketing).


# üî¨ Fase 2: Investiga√ß√£o (Learning Resources & Activities) 

Esta √© a fase de aquisi√ß√£o de conhecimento e desenvolvimento das habilidades necess√°rias para superar o Desafio.


## T√≥pico (Habilidade):

- Python Intermedi√°rio (Scripts e Automa√ß√£o) 

- Processamento de M√∫ltiplas Fontes (ETL)

- GIT para Controle de Vers√£o

- Organiza√ß√£o e Documenta√ß√£o

- Extra√ß√£o de Insights e Relat√≥rios


## Atividades de Estudo Recomendadas:

- Estruturas de Dados Avan√ßadas, List Comprehensions, itertools, Fun√ß√µes e Classes, Manipula√ß√£o de Arquivos e Caminhos (os, pathlib).

- Pandas: Dominar read_csv, read_json, merge, groupby, apply, e tratamento de valores ausentes (NaN).

- SQL: Praticar SELECT, FROM, WHERE, JOIN (SQLite local). APIs: Praticar requisi√ß√µes HTTP (requests).

- Fun√ß√µes e Classes: Praticar a escrita de scripts modulares e reutiliz√°veis (automa√ß√£o de tarefas).

- Extra√ß√£o de APIs: Praticar requisi√ß√µes HTTP (biblioteca requests) para obter dados em formato JSON.

- Comandos Essenciais: init, clone, add, commit, push, pull, branch, merge. Fluxo de Trabalho com feature branches.

- Boas Pr√°ticas: Aprender a usar docstrings (documenta√ß√£o de fun√ß√µes), coment√°rios claros e type hinting.

- Estrutura de Projeto: Organizar em pastas l√≥gicas (src, data, notebooks).

- Visualiza√ß√£o B√°sica: matplotlib, seaborn ou Plotly (foco em comunica√ß√£o clara).

- Estat√≠stica Descritiva: Calcular m√©dias, desvios e identificar outliers


## Recursos de Aprendizagem Sugeridos:

- Livros/Documenta√ß√£o do Python sobre estruturas e bibliotecas padr√µes, Guias de estilo de c√≥digo (PEP 8).
 
- Cursos online (DataCamp, Coursera) com foco em Pandas e ETL. Pr√°tica no Kaggle (limpeza de dados).

- Pr√°tica no Kaggle (limpeza de dados).

- Tutoriais sobre a biblioteca requests e json no Python.

- Exerc√≠cios pr√°ticos de SQL online (SQL Zoo, HackerRank).

- Projetos que simulem a leitura de dados de CSV, JSON e DB ao mesmo tempo.

- Curso b√°sico de Git e GitHub (ex: FreeCodeCamp, Alura).
 
- Utilizar Git em todos os projetos de Python desenvolvidos na Fase 2.

- Leitura sobre melhores pr√°ticas de documenta√ß√£o de c√≥digo em Python.

- Tutoriais de visualiza√ß√£o de dados em Python (foco em comunica√ß√£o clara).

- An√°lise de datasets de exemplo (ex: dados de vendas) para encontrar tend√™ncias.


# ‚ú® Fase 3: A√ß√£o (Solution Development & Reflection)

Aplica√ß√£o pr√°tica do conhecimento adquirido, culminando na solu√ß√£o do Desafio e reflex√£o sobre o aprendizado.

## Etapa 1: Desenvolver a Solu√ß√£o do Desafio

### A√ß√£o: 

Constru√ß√£o do Mini-Pipeline de ETL


### Extra√ß√£o (E):

Criar tr√™s fontes de dados simuladas: um CSV de pedidos, um JSON de informa√ß√µes do cliente (extra√≠do via simula√ß√£o de API), e um pequeno banco de dados SQLite com dados de estoque (consulta SQL b√°sica).

### Transforma√ß√£o (T):

Escrever um script Python (usando Pandas) que leia, integre (merge), limpe (trate NaNs e formate tipos) e transforme os dados das tr√™s fontes em um dataset √∫nico e coerente.


### Automa√ß√£o: 

Garantir que este processo seja executado por um √∫nico script modular.


### Carga e An√°lise (L & Insights):

Carregar o dataset limpo em um arquivo final (ex: clean_data.csv).

Desenvolver o segundo script de an√°lise para extrair um insight claro (ex: "Qual a categoria de produto com maior margem, mas com o maior tempo m√©dio de entrega?") e gerar um gr√°fico de visualiza√ß√£o (relat√≥rio simples).


### Controle de Vers√£o:

Usar o Git desde o in√≠cio, com branches (feature/extract, feature/transform, feature/analysis) e commits regulares e detalhados.

## Etapa 2: Compartilhamento e Documenta√ß√£o

### Documenta√ß√£o: 

Escrever um README.md claro no reposit√≥rio Git, explicando o prop√≥sito do projeto, como executar o script (instru√ß√µes), e o insight final encontrado.

### Organiza√ß√£o: 

Organizar o reposit√≥rio seguindo a Estrutura de Projeto (ex: /src para scripts, /data para dados brutos e limpos).


## ‚ú® Etapa 3: Reflex√£o (Otimiza√ß√£o e Melhoria) - Mentalidade de Qualidade Total

Foco no Processo, Preven√ß√£o, e Orienta√ß√£o ao Cliente (o usu√°rio do insight).

### Responder √†s seguintes perguntas para consolidar o aprendizado:

1. Foco na Qualidade e Robustez do Processo (Preven√ß√£o de Defeitos)

O que aprendi sobre lidar com tipos de dados conflitantes de m√∫ltiplas fontes?

TQM (Preven√ß√£o): Como a detec√ß√£o precoce de conflitos de dados no pipeline (T) melhora a confiabilidade do dataset final, prevenindo erros de an√°lise?

Lidar com tipos de dados como strings representando n√∫meros ('10.50') em CSV e floats em JSON exigiu a defini√ß√£o de um padr√£o de tipo √∫nico logo na fase de Transforma√ß√£o. 
Essa padroniza√ß√£o inicial atua como um Ponto de Controle de Qualidade (QC). Se este passo fosse ignorado, an√°lises subsequentes (como a m√©dia de valores ou c√°lculos de margem) gerariam resultados incorretos ou erros de execu√ß√£o. O aprendizado √© que a Transforma√ß√£o deve priorizar a consist√™ncia e a coer√™ncia dos dados antes de qualquer agrega√ß√£o, aplicando o princ√≠pio de que √© mais barato corrigir o erro na fonte (Extra√ß√£o/Transforma√ß√£o) do que na an√°lise final.

2. Controle de Vers√£o como Gest√£o da Qualidade e Risco

Onde o uso do Git me salvou de um problema?

TQM (Rastreabilidade e Melhoria): Como o Git assegurou a rastreabilidade do meu c√≥digo, facilitando auditorias e permitindo a melhoria iterativa e segura do pipeline?

O Git n√£o apenas "salva", mas garante a integridade e auditabilidade do projeto, um pilar do TQM. 
O uso de feature branches (ex: feature/extract) impediu que o desenvolvimento da l√≥gica de integra√ß√£o dos dados (muitas vezes complexa e propensa a erros) contaminasse o c√≥digo principal de an√°lise (main). 
Em um ponto, a l√≥gica de merge estava incorreta, gerando duplicatas. 
Gra√ßas aos commits regulares, pude reverter rapidamente para a vers√£o est√°vel anterior (rollback), corrigindo o erro sem introduzir um novo "defeito" no produto final.
Isso demonstra a capacidade de Gerenciamento de Configura√ß√£o e Qualidade no Desenvolvimento.

3. Otimiza√ß√£o do Fluxo de Trabalho (Melhoria Cont√≠nua - Kaizen)

Como meu script de automa√ß√£o poderia ser melhorado para ser mais r√°pido ou robusto (ex: tratamento de erros)?

TQM (Efici√™ncia e Zero Defeito): Quais otimiza√ß√µes podem ser aplicadas ao c√≥digo para aumentar a efici√™ncia (velocidade) e a robustez (toler√¢ncia a falhas) do pipeline, aproximando-o do "Zero Defeito"?

Para aumentar a robustez, o pipeline precisa de tratamento de exce√ß√µes (try...except) para falhas de conex√£o (API ou SQL). 
Se a API de clientes falhar, o script deve registrar o erro e talvez carregar apenas as fontes dispon√≠veis, em vez de travar o processo inteiro. 
Para efici√™ncia, se o dataset crescesse, a substitui√ß√£o de loops em Python por opera√ß√µes vetorizadas do Pandas (apply) seria essencial. 
A melhoria cont√≠nua (Kaizen) aqui significa planejar a arquitetura de tal forma que ela escale (desempenho) e tolere falhas (robustez), n√£o apenas entregue o resultado uma √∫nica vez.

4. Alinhamento Estrat√©gico e Valor para o Cliente

O insight gerado pode realmente levar a uma melhoria processual na empresa hipot√©tica?

TQM (Orienta√ß√£o ao Cliente/Stakeholder): O insight gerado √© acion√°vel e agrega valor estrat√©gico ao "cliente" (a √°rea de neg√≥cios que usar√° o painel)?

O insight encontrado ("A categoria de produto de maior margem tem o maior tempo m√©dio de entrega") √© diretamente acion√°vel. 
Ele transforma um dado descritivo em uma oportunidade de melhoria processual. 
O "cliente" (Log√≠stica) pode agora investigar se o atraso est√° no picking, packing ou no transporte. 
Este resultado demonstra que o pipeline est√° alinhado com a estrat√©gia de neg√≥cio, entregando um produto de alta qualidade (relev√¢ncia) que resolve uma "dor" da opera√ß√£o, ao inv√©s de apenas gerar gr√°ficos bonitos.
